{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 4 - WS 2023 -->\n",
    "\n",
    "# Adaptive Optimisation (22 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the fourth assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless it is explicitly requested!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with some of the most common (adaptive) **optimisation algorithms**. Essentially, the most common optimisation algorithms are nothing more than variants of gradient descent. Although it is often claimed that stochastic gradient descent outperforms any adaptive learning method when carefully configured, it is often more convenient to use a method that requires less tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nnumpy import Module, Parameter\n",
    "from nnumpy import LossFunction, Mean\n",
    "from nnumpy import Optimiser\n",
    "from nnumpy.data import get_mnist_data\n",
    "from nnumpy.utils import split_data, to_one_hot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredErrorLoss(LossFunction):\n",
    "    \"\"\" Module for squared error loss. \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mean = Mean(axis=-1)\n",
    "    \n",
    "    def raw_outputs(self, pred, targ):\n",
    "        diffs, m_cache = self.mean.compute_outputs(pred - targ)\n",
    "        return diffs ** 2 / 2., (diffs, m_cache)\n",
    "\n",
    "    def raw_grads(self, grads, cache):\n",
    "        diffs, m_cache = cache\n",
    "        dpred = self.mean.compute_grads(grads * diffs, m_cache)\n",
    "        return dpred, -dpred\n",
    "    \n",
    "    \n",
    "class Scale(Module):\n",
    "    \"\"\" Simple scaling module for testing. \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_parameter(\"scale\", np.ones(1))\n",
    "    \n",
    "    def compute_outputs(self, x):\n",
    "        return self.scale * x, x\n",
    "    \n",
    "    def compute_grads(self, grads, x):\n",
    "        self.scale.grad = x.ravel() @ grads.ravel()\n",
    "        return self.scale * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    \"\"\"\n",
    "    Compute accuracy for classification network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : ndarray\n",
    "        The logit-predictions from the network.\n",
    "    labels : ndarray\n",
    "        The target labels for the task.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    acc : float\n",
    "        The fraction of correctly classified samples.\n",
    "    \"\"\"\n",
    "    idx = np.argmax(logits, axis=1)\n",
    "    return np.mean(labels[np.arange(len(idx)), idx])\n",
    "\n",
    "    \n",
    "def plot_curves(model, train_errors, valid_errors):\n",
    "    \"\"\"\n",
    "    Plot learning curves\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The network to plot learning curves for.\n",
    "    train_errors : ndarray\n",
    "        The training errors for each batch in every epoch.\n",
    "    valid_errors : ndarray\n",
    "        The validation errors after each epoch.\n",
    "    \"\"\"\n",
    "    plt.title(\"learning curves\")\n",
    "    train_acc, = evaluate(model, accuracy, Dataloader(*train_data))\n",
    "    loss_curve, = plt.semilogy(np.mean(train_errors, axis=1), \n",
    "                           label=f'train (acc: {100 * train_acc:2.2f}%)')\n",
    "    valid_acc, = evaluate(model, accuracy, Dataloader(*valid_data))\n",
    "    plt.semilogy(valid_errors, linestyle='--', color=loss_curve.get_color(), \n",
    "             label=f'valid (acc: {100 * valid_acc:2.2f}%)')\n",
    "    plt.legend()\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you should know by now, the *backpropagation* algorithm is little more than a combination of the chain rule and some form of gradient descent. Although this happens to work well in practice, it is good to be aware of possible issues when using first-order optimisation methods:\n",
    "\n",
    " 1. Gradient descent can/will get stuck in *local minima*.\n",
    " 2. The *gradient magnitude* tells you nothing about how far away minima are.\n",
    " 3. When optimising the *empirical error*, gradient descent would require the gradient over the entire dataset.\n",
    "\n",
    "Note that this last point is not necessarily an issue, but it is useful to keep in mind. Also, it implies that the gradients that can be computed on the entire dataset do not need to correspond to the gradients that would be required to minimise the generalisation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Stochasticity (3 Points)\n",
    "\n",
    "Rather than using plain gradient descent algorithm, a stochastic variant is used to train neural networks. This variant is known as *Stochastic Gradient Descent*, or *SGD* for short. Although this naming scheme seems to suggest that stochasticity is part of the algorithm, it is actually introduced by how we use the data to compute gradients.\n",
    "\n",
    "Instead of computing the gradients over the entire dataset in one go, the samples in the dataset are split up in more manageable pieces called *mini-batches*. This can speed up the computations significantly and avoids memory problems with very large datasets. Another benefit from mini-batches is that they introduce variation, or *stochasticity*, in the gradient computations. After all, the gradient for each mini-batch will be different to the gradient for other mini-batches or for all samples. This stochasticity can be useful to escape local minima in the optimisation process. To amplify this stochasticity, it is also common to shuffle the samples in the dataset so that mini-batches consist of different samples.\n",
    "\n",
    "> Complete the `Dataloader` class below to process the data in mini-batches of pre-specified size. Also make sure to shuffle the data to get more stochasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some Notes on python generators\n",
    "\n",
    "In python, a [generator](https://wiki.python.org/moin/Generators) is a function with some state that can return multiple values. You probably have already used generators without realising it. Probably, the most famous generator is `range`, which could be defined as follows:\n",
    "```python\n",
    "def _range(start, stop, step=1):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step\n",
    "```\n",
    "\n",
    "Notice the `yield` keyword. This has a similar effect as `return` in that it provides a value to the outer scope of the function. However, it does not cause the function to be exited. Instead, the current state in the function is stored until the next value is requested. To get the return values of a generator, there are essentially two options:\n",
    " 1. Using the `next` function. This will simply run the function until the next `yield` statement and give back the yielded value.\n",
    " 2. By iterating over the generator in any way. This will consequently call `next` on the generator until the function exits.\n",
    " \n",
    "For more information, please refer to the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff4cd772cbc136571f51d9be86d74a8a",
     "grade": false,
     "grade_id": "cell-098ac01eaf374ea1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    \n",
    "    def __init__(self, x, y, batch_size=None, shuffle=False, seed=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, ...) ndarray\n",
    "            the `N` input samples in the dataset\n",
    "        y : (N, ...) ndarray\n",
    "            the `N` output samples in the dataset\n",
    "        batch_size : int, optional\n",
    "            number of samples to include in a single mini-batch.\n",
    "        shuffle : bool, optional\n",
    "            whether or not the data should be shuffled.\n",
    "        seed : int, optional\n",
    "            seed for the pseudo random number generator used for shuffling.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = len(x) if batch_size is None else int(batch_size)\n",
    "        self.shuffle = shuffle\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterates over the samples of the data.\n",
    "        \n",
    "        Yields\n",
    "        ------\n",
    "        x : ndarray\n",
    "            input features for the batch\n",
    "        y : ndarray\n",
    "            target values for the batch\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        Each batch should contain the specified number of samples,\n",
    "        except for the last batch if the batch_size does \n",
    "        not divide the number of samples in the data.\n",
    "        \"\"\"\n",
    "        x, y = np.copy(self.x), np.copy(self.y)\n",
    "        # YOUR CODE HERE\n",
    "        if self.shuffle:\n",
    "            indices = self.rng.permutation(len(x))\n",
    "            x, y = x[indices], y[indices]\n",
    "        for i in range(0, len(x), self.batch_size):\n",
    "            yield x[i:i+self.batch_size], y[i:i+self.batch_size]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return -(-len(self.x) // self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a2fb992e046ca0b58fee5f5608faa06",
     "grade": true,
     "grade_id": "cell-a1d8c6dccdc950c4",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete! \n",
    "data = np.arange(7)\n",
    "data_loader = Dataloader(data, np.copy(data), batch_size=3, shuffle=False)\n",
    "xy = next(iter(data_loader))\n",
    "assert isinstance(xy, tuple), (\n",
    "    \"ex1: the dataloader does not yield tuples (-0.5 points)\"\n",
    ")\n",
    "assert len(xy) == 2, (\n",
    "    \"ex1: the dataloader does not yield inputs and outputs (-0.5 points)\"\n",
    ")\n",
    "\n",
    "x_batch, y_batch = xy\n",
    "assert isinstance(x_batch, np.ndarray), (\n",
    "    \"ex1: the input batch yielded by the dataloader is not a numpy array (-0.5 points)\"\n",
    ")\n",
    "assert isinstance(y_batch, np.ndarray), (\n",
    "    \"ex1: the label batch yielded by the dataloader is not a numpy array (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abb4cc4c45ab0fa1efcf29a0049375a5",
     "grade": true,
     "grade_id": "cell-153213c022da0956",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "data = np.arange(7)\n",
    "data_loader = Dataloader(data, np.copy(data), batch_size=3, shuffle=False)\n",
    "x_batch, y_batch = next(iter(data_loader))\n",
    "assert len(x_batch) == 3, (\n",
    "    \"ex1: number of samples in input batch is not correct (-0.5 points)\"\n",
    ")\n",
    "assert len(y_batch) == 3, (\n",
    "    \"ex1: number of samples in label batch is not correct (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69bf4e627d3d6aa31dc9ae36dcf247ba",
     "grade": true,
     "grade_id": "cell-1a0517a0e717401b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete! \n",
    "data = np.arange(7)\n",
    "data_loader = Dataloader(data, np.copy(data), batch_size=3, shuffle=True, seed=17)\n",
    "x_batch, y_batch = next(iter(data_loader))\n",
    "assert isinstance(x_batch, np.ndarray), (\n",
    "    \"ex1: the shuffled input batch yielded by the dataloader is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert isinstance(y_batch, np.ndarray), (\n",
    "    \"ex1: the shuffled label batch yielded by the dataloader is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert len(x_batch) == 3, (\n",
    "    \"ex1: number of samples in shuffled input batch is not correct (-1 point)\"\n",
    ")\n",
    "assert len(y_batch) == 3, (\n",
    "    \"ex1: number of samples in shuffled label batch is not correct (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9026aa1d74f5ad528cc28c93c9d7bf3f",
     "grade": true,
     "grade_id": "cell-e7e4e4ecf2afc4c4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Gradient Descent with Momentum (3 Points)\n",
    "\n",
    "Another way to keep gradient descent from getting stuck in local minima is to use momentum. Momentum accumulates the gradient directions over different batches and accelerates/decelarates the descent when all gradients point in the same/different direction(s). This also means that the update does not directly use the magnitude of the gradient, but instead focuses on the direction. \n",
    "\n",
    "> Implement the `get_direction` and `init_state` methods for the gradient descent optimiser with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "589765c0ab0f9725d7d2ce12df8e659e",
     "grade": false,
     "grade_id": "cell-a174cbe419d5ee48",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GradientDescent(Optimiser):\n",
    "    \"\"\" NNumpy implementation of gradient descent. \"\"\"\n",
    "\n",
    "    def __init__(self, parameters, lr: float, momentum: float = 0.):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        momentum : float\n",
    "            Momentum term for the gradient descent.\n",
    "        \"\"\"\n",
    "        self.mu = momentum\n",
    "        super().__init__(parameters, lr)\n",
    "\n",
    "    def init_state(self, par):\n",
    "        \"\"\"\n",
    "        Create the initial optimiser state for a parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        par : Parameter\n",
    "            The parameter to create the initial state for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        state : object or tuple of objects\n",
    "            The initial optimiser state for the given parameter.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return par\n",
    "        \n",
    "    def get_direction(self, grad, state):\n",
    "        \"\"\"\n",
    "        Compute the update direction from gradient and state for single parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grad : ndarray\n",
    "            Gradient for the parameter to update.\n",
    "        state : object or tuple of objects\n",
    "            State information that is necessary to compute the update direction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        direction : ndarray\n",
    "            The update direction according to the algorithm.\n",
    "        new_state: object or tuple of objects\n",
    "            Updated state information after computing the update direction.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "#         # m(t) = bm(t-1) + (1-b)*g(t)         \n",
    "        direction = self.mu * state - (1 - self.mu) * grad\n",
    "        new_state = state\n",
    "        return direction, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca7d75b4cdbb9f9583750083310e18a5",
     "grade": true,
     "grade_id": "cell-9b64f5eeaee158b2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "par = Parameter(np.zeros(10))\n",
    "gd = GradientDescent([par], lr=1e-2)\n",
    "state0 = gd.init_state(par)\n",
    "out = gd.get_direction(np.zeros_like(par), state0)\n",
    "assert isinstance(out, tuple) and len(out) == 2, (\n",
    "    \"ex2: get_direction does not return tuple with direction and new_state (-1 point)\"\n",
    ")\n",
    "direction, new_state = out\n",
    "assert isinstance(direction, np.ndarray), (\n",
    "    \"ex2: get_direction does not return direction as numpy array (-1 point)\"\n",
    ")\n",
    "assert type(state0) == type(new_state), (\n",
    "    \"ex2: get_direction returns inconsistent state object (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7e8bcc7de8e6f764bd5114b5c72c2e8",
     "grade": true,
     "grade_id": "cell-5edd409ce58ffd5a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "par = Parameter(np.zeros(10))\n",
    "gd = GradientDescent([par], lr=1e-2, momentum=0.9)\n",
    "state0 = gd.init_state(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e1bf3c48f376d2702f83e45c5391c17",
     "grade": true,
     "grade_id": "cell-86dac3cac8e9b70a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "par = Parameter(np.zeros(10))\n",
    "gd = GradientDescent([par], lr=1e-2, momentum=0.9)\n",
    "state0 = gd.init_state(par)\n",
    "direction, new_state = gd.get_direction(np.zeros_like(par), state0)\n",
    "assert isinstance(direction, np.ndarray), (\n",
    "    \"ex2: get_direction with momentum does not return direction as numpy array (-1 point)\"\n",
    ")\n",
    "assert type(state0) == type(new_state), (\n",
    "    \"ex2: get_direction with momentum returns inconsistent state object (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: The Adamax Optimiser (5 Points)\n",
    "\n",
    "Momentum already provides a way to reduce the importance of the gradient magnitude. \n",
    "With adaptive learning rate methods, an attempt is made to ignore most of the magnitude information and the size of the update is mainly controlled by controlling the learning rate. \n",
    "One of the most popular first order adaptive methods in practice, is the Adam optimiser.\n",
    "However, Adam also has a sibling called *Adamax* that typically performs on a similar level.\n",
    "\n",
    "To understand Adamax, we have to look at the variance as a rescaled $L_2$ norm.\n",
    "Instead of dividing by the $L_2$ norm, as it is done in Adam, Adamax divides by the $L_\\inf$ norm.\n",
    "This gives rise to the following updates for first and second moment estimates:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\boldsymbol{m}^{(t)} &= \\beta_1 \\boldsymbol{m}^{(t-1)} + (1 - \\beta_1) \\boldsymbol{g}^{(t)} \\\\\n",
    "    \\boldsymbol{u}^{(t)} &= \\max\\Bigl(\\beta_2 \\boldsymbol{u}^{(t-1)}, \\bigl|\\boldsymbol{g}^{(t)}\\bigr|\\Bigr),\n",
    "\\end{aligned}$$\n",
    "\n",
    "giving rise to the update:\n",
    "\n",
    "$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta \\frac{1}{\\boldsymbol{u}^{(t)} + \\epsilon} \\odot \\boldsymbol{m}^{(t)}.$$\n",
    "\n",
    "> Implement the `get_direction` and `init_state` methods for the Adamax optimisation algorithm.\n",
    "> Also, make sure to implement a bias correction for the exponential moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "936c9bb97b60d26ec3086313b3e9229d",
     "grade": false,
     "grade_id": "cell-83dc8a5ab05cec48",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Adamax(Optimiser):\n",
    "    \"\"\" NNumpy implementation of the Adam algorithm. \"\"\"\n",
    "\n",
    "    def __init__(self, parameters, lr: float = 1e-3, betas: tuple = (.9, .999),\n",
    "                 epsilon: float = 1e-7, bias_correction=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        betas : tuple of 2 floats, optional\n",
    "            Decay factors for the exponential averaging of mean, resp. variance.\n",
    "        epsilon : float, optional\n",
    "            Small number that is added to denominator for numerical stability.\n",
    "        bias_correction : bool, optional\n",
    "            Whether or not mean and bias estimates should be bias-corrected.\n",
    "        \"\"\"\n",
    "        beta1, beta2 = betas\n",
    "        self.beta1 = float(beta1)\n",
    "        self.beta2 = float(beta2)\n",
    "        self.eps = float(epsilon)\n",
    "        self.bias_correction = bias_correction\n",
    "        super().__init__(parameters, lr)\n",
    "\n",
    "    def init_state(self, par):\n",
    "        \"\"\"\n",
    "        Create the initial optimiser state for a parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        par : Parameter\n",
    "            The parameter to create the initial state for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        state : object or tuple of objects\n",
    "            The initial optimiser state for the given parameter.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        self.t = 0\n",
    "        return (np.ones_like(par.data), np.zeros_like(par.data))\n",
    "\n",
    "    def get_direction(self, grad, state):\n",
    "        \"\"\"\n",
    "        Compute the update direction from gradient and state for single parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grad : ndarray\n",
    "            Gradient for the parameter to update.\n",
    "        state : object or tuple of objects\n",
    "            State information that is necessary to compute the update direction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        direction : ndarray\n",
    "            The update direction according to the algorithm.\n",
    "        new_state: object or tuple of objects\n",
    "            Updated state information after computing the update direction.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        m, u = state\n",
    "        m = self.beta1 * m + (1 - self.beta1) * grad\n",
    "        u = np.maximum(self.beta2 * u, np.abs(grad))\n",
    "        \n",
    "        self.t = self.t + 1\n",
    "        if self.bias_correction:\n",
    "            m_hat = m / (1 - self.beta1 ** self.t)\n",
    "        else:\n",
    "            m_hat = m\n",
    "            \n",
    "        #direction = m_hat / (u + self.eps)\n",
    "        direction = m / (u + self.eps)\n",
    "        new_state = (m, u)\n",
    "        \n",
    "        return direction, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a9019644df362fcd2096d242d15d234",
     "grade": true,
     "grade_id": "cell-420e31a909cffd40",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "par = Parameter(np.zeros(10))\n",
    "adamax = Adamax([par])\n",
    "state0 = adamax.init_state(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "529fe57cf62d846d0f1ca0151fdc253d",
     "grade": true,
     "grade_id": "cell-ba8208e654c4a1a3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "par = Parameter(np.zeros(10))\n",
    "adamax = Adamax([par])\n",
    "state0 = adamax.init_state(par)\n",
    "out = adamax.get_direction(np.zeros_like(par), state0)\n",
    "assert isinstance(out, tuple) and len(out) == 2, (\n",
    "    \"ex3: get_direction does not return tuple with direction and new_state (-1 point)\"\n",
    ")\n",
    "direction, new_state = out\n",
    "assert isinstance(direction, np.ndarray), (\n",
    "    \"ex3: get_direction does not return direction as numpy array (-1 point)\"\n",
    ")\n",
    "assert type(state0) == type(new_state), (\n",
    "    \"ex3: get_direction returns inconsistent state object (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80ec17f9470cdf7f8dc89ce21bb7ec7",
     "grade": true,
     "grade_id": "cell-16526613cb5b1660",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "par = Parameter(np.zeros(10))\n",
    "adamax = Adamax([par])\n",
    "state0 = adamax.init_state(par)\n",
    "grad = np.ones_like(par)\n",
    "dir1, state1 = adamax.get_direction(grad, state0)\n",
    "dir2, state2 = adamax.get_direction(grad, state1)\n",
    "dir3, _ = adamax.get_direction(grad, state2)\n",
    "assert np.allclose(dir3, grad), (\n",
    "    \"ex3: get_direction returns wrong direction for third update when gradients are one (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e9b9d4bebf1e289cabe3cd118d66813",
     "grade": true,
     "grade_id": "cell-830c538a835d12ac",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75b52dec805074c0da19477bb80aac80",
     "grade": true,
     "grade_id": "cell-24c347fc3d650512",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have all components that are necessary to start training neural networks.\n",
    "With the modules you created throughout this semester, you can already build a wide variety of network architectures.\n",
    "Together with the optimisers from this assignment, you can start training networks in your very own DL framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e807e54eae877f05e6d53a4bff254623",
     "grade": false,
     "grade_id": "cell-34c4e3e971be2abd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the modules you have written in previous assignments,\n",
    "# or alternatively, the code for these modules, in this box.\n",
    "# e.g. from nnumpy import Sequential, Linear, Conv2d, MaxPool2d, Tanh, LogitCrossEntropy\n",
    "import numpy as np\n",
    "\n",
    "from nnumpy import Module, Container, LossFunction\n",
    "from nnumpy.testing import gradient_check\n",
    "from nnumpy.utils import to_one_hot\n",
    "\n",
    "rng = np.random.default_rng(1856)\n",
    "\n",
    "class Sequential(Container):\n",
    "    \"\"\"\n",
    "    NNumpy module that chains together multiple one-to-one sub-modules.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    Doubling a module could be done as follows:\n",
    "    >>> module = Module()\n",
    "    >>> seq = Sequential(module, module)\n",
    "    \n",
    "    Modules can be accessed by index or by iteration:\n",
    "    >>> assert module is seq[0] and module is seq[1]\n",
    "    >>> mod1, mod2 = (m for m in seq)\n",
    "    >>> assert mod1 is module and mod2 is module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *modules):\n",
    "        super().__init__()\n",
    "        if len(modules) == 1 and hasattr(modules[0], '__iter__'):\n",
    "            modules = modules[0]\n",
    "        \n",
    "        for mod in modules:\n",
    "            self.add_module(mod)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, D) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        cache = []\n",
    "        y = []\n",
    "        current_x = x\n",
    "        current_cache = []\n",
    "\n",
    "        for mod in self._modules:\n",
    "            current_x, current_cache = mod.compute_outputs(current_x.T)\n",
    "            cache.append(current_cache)\n",
    "            \n",
    "        y = current_x\n",
    "        return y, cache\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, D) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        self._modules.reverse()\n",
    "        cache.reverse()\n",
    "        current_dx = grads\n",
    "        \n",
    "        for mod,c in zip(self._modules,cache):\n",
    "            current_dx = mod.compute_grads(current_dx, c)\n",
    "            current_dx = current_dx.T\n",
    "        \n",
    "        dx = current_dx\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    NNumpy implementation of a fully connected layer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features (D) this layer expects.\n",
    "    out_features : int\n",
    "        Number of output features (K) this layer expects.\n",
    "    use_bias : bool\n",
    "        Flag to indicate whether the bias parameters are used.\n",
    "\n",
    "    w : Parameter\n",
    "        Weight matrix.\n",
    "    b : Parameter\n",
    "        Bias vector.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> fc = Linear(10, 1)\n",
    "    >>> fc.reset_parameters()  # init parameters\n",
    "    >>> s = fc.forward(np.random.randn(1, 10))\n",
    "    >>> fc.zero_grad()  # init parameter gradients\n",
    "    >>> ds = fc.backward(np.ones_like(s))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # register parameters 'w' and 'b' here (mind use_bias!)\n",
    "        # YOUR CODE HERE\n",
    "        self.register_parameter(\"w\", np.zeros((self.out_features, self.in_features)))\n",
    "        if(self.use_bias):\n",
    "            self.register_parameter(\"b\", np.zeros((self.out_features)))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self, seed: int = None):\n",
    "        \"\"\" \n",
    "        Reset the parameters to some random values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int, optional\n",
    "            Seed for random initialisation.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.w = rng.standard_normal(size=self.w.shape)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros_like(self.b)\n",
    "    \n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, D) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        s : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        N = x.shape[0]\n",
    "        D = x.shape[1]\n",
    "        K = self.w.shape[0]\n",
    "        s = np.zeros((N,K))\n",
    "        \n",
    "        if(self.use_bias):\n",
    "            #s = x @ self.w.T + self.b\n",
    "            s = np.dot(x,self.w.T) + self.b\n",
    "        else:\n",
    "            #s = x @ self.w.T\n",
    "            s = np.dot(x,self.w.T)\n",
    "            \n",
    "        cache = x\n",
    "        return s, cache\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, D) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        D = self.w.shape[1]\n",
    "        N = grads.shape[0]\n",
    "        K = grads.shape[1]\n",
    "        dx = np.zeros((N,D))\n",
    "        \n",
    "        #dx = grads @ self.w\n",
    "        dx = np.dot(grads,self.w)\n",
    "        #dw = grads.T @ cache\n",
    "        dw = np.dot(grads.T,cache)\n",
    "        if(self.use_bias):\n",
    "            db = np.sum(grads, axis=0)\n",
    "        else:\n",
    "            db = np.zeros(self.b.shape)\n",
    "            \n",
    "        self.b.grad = db\n",
    "        self.w.grad = dw\n",
    "        \n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class ReLU(Module):\n",
    "    \"\"\" NNumpy implementation of the Rectified Linear Unit. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \"\"\"\n",
    "        a = np.zeros((s.shape[0],s.shape[1]))\n",
    "        #for i in range(s.shape[0]):\n",
    "        #    for j in range(s.shape[1]):\n",
    "        #        if(s[i][j] > 0):\n",
    "        #            a[i][j] = s[i][j]\n",
    "        if(s.all() > 0):\n",
    "            a = s\n",
    "        \"\"\"\n",
    "        \n",
    "        shape = s.shape\n",
    "        s = s.flatten()\n",
    "        a = np.zeros(s.shape)\n",
    "        \n",
    "        for i in range(s.shape[0]):\n",
    "            if(s[i] > 0):\n",
    "                a[i] = s[i]\n",
    "        \n",
    "        a = a.reshape(shape)\n",
    "        s = s.reshape(shape)\n",
    "        \n",
    "        return a, s\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \"\"\"\n",
    "        ds = np.zeros((grads.shape[0],grads.shape[1]))\n",
    "        for i in range(grads.shape[0]):\n",
    "            for j in range(grads.shape[1]):\n",
    "                if(grads[i][j] > 0):\n",
    "                    ds[i][j] = 1\n",
    "        #if(grads.all() > 0):\n",
    "        #    ds = 1\n",
    "        \"\"\"\n",
    "        shape = grads.shape\n",
    "        grads = grads.flatten()\n",
    "        cache = cache.flatten()\n",
    "        ds = np.zeros(grads.shape)\n",
    "        \n",
    "        for i in range(grads.shape[0]):\n",
    "            dss = grads[i] * cache[i]\n",
    "            if(dss > 0):\n",
    "                ds[i] = 1\n",
    "        \n",
    "        ds = ds.reshape(shape)\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    \n",
    "class Tanh(Module):\n",
    "    \"\"\" NNumpy implementation of the hyperbolic tangent function. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return np.tanh(s),s\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return (1 - np.tanh(cache) * np.tanh(cache)) * grads\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Module):\n",
    "    \"\"\" NNumpy module to convert multi-dimensional outputs to a single vector. \"\"\"\n",
    "    \n",
    "    def compute_outputs(self, x):\n",
    "        return x.reshape(len(x), -1), x.shape\n",
    "    \n",
    "    def compute_grads(self, grads, shape):\n",
    "        return grads.reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the remainder of this assignment will be to train a network on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 32, 32) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "def process_data(x_train, y_train):\n",
    "    x_train = x_train.reshape(-1, 1, 28, 28)\n",
    "    y_train = to_one_hot(y_train)\n",
    "    \n",
    "    # NOTE: custom data processing is allowed\n",
    "    x_train = np.pad(x_train, ((0, 0), (0, 0), (2, 2), (2, 2)))\n",
    "    x_train = x_train - np.mean(x_train, axis=(1, 2, 3), keepdims=True)\n",
    "    x_train = x_train / np.std(x_train, axis=(1, 2, 3), keepdims=True)\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "x_train, y_train = get_mnist_data()\n",
    "x_train, y_train = process_data(x_train, y_train)\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Evaluation and Update (3 Points)\n",
    "\n",
    "When using the optimisers to fit a neural network to a given set of data, we can effectively minimise the empirical error. However, we are actually interested in minimising the risk. Therefore, it is also useful to evaluate the network regularly on unseen data.\n",
    "\n",
    " > Implement the `evaluate`, and `update` functions so that they perform the training and evaluation computations, respectively, for one iteration (aka *epoch*) over the entire dataset. Make sure to return the loss values to get loss curves at the end of this assignment.\n",
    " \n",
    "**Hint:** you can use the `step` method of the optimiser to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c918e8a90adf7568022981e2741ac30a",
     "grade": false,
     "grade_id": "cell-19ef0b9a834d55fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(network, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Evaluate a network by computing a metric for specific data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    metric : callable\n",
    "        A function that takes logits and labels \n",
    "        and returns a scalar numpy array.\n",
    "    data_loader : Dataloader\n",
    "        The data loader that provides the batches.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    values : ndarray\n",
    "        The computed metric values for each batch in the data loader.\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    values = []\n",
    "    \n",
    "    for x, y in data_loader:\n",
    "        logits = network(x)\n",
    "        loss = metric(logits, y)\n",
    "        values.append(loss)\n",
    "    \n",
    "    return np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3a8397b518ec62c82a17d0983a35ed0",
     "grade": true,
     "grade_id": "cell-e428510b42bb767b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "data = np.linspace(-3, 3, 24).reshape(4, 2, 3)\n",
    "loader = Dataloader(data, data.reshape(len(data), -1), batch_size=2)\n",
    "net = Flatten()\n",
    "loss = MeanSquaredErrorLoss()\n",
    "result = evaluate(net, loss, loader)\n",
    "assert isinstance(result, np.ndarray), (\n",
    "    \"ex4: output of evaluate is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert result.size == 2, (\n",
    "    \"ex4: output of evaluate does not hold scalar result for each mini-batch (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca4bb7f187e92ffbe2fb5519fb1d63b1",
     "grade": true,
     "grade_id": "cell-c3169d0d6193e2c7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b713b6186f5695bdc2f1a35953a7437",
     "grade": false,
     "grade_id": "cell-eb6fe48dec40e037",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update(network, loss, data_loader, optimiser):\n",
    "    \"\"\"\n",
    "    Update a network by optimising the loss for the given data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    loss : Module\n",
    "        Loss function module.\n",
    "    data_loader : Dataloader\n",
    "        The data loader that provides the batches.\n",
    "    optimiser : Optimiser\n",
    "        Optimisation algorithm to use for the update.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    errors : ndarray\n",
    "        The computed loss for each batch in the data loader.\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    loss.train()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    for ind, (inputs, labels) in enumerate(data_loader):\n",
    "        network.zero_grad()\n",
    "        \n",
    "        outputs = network.compute_outputs(inputs)\n",
    "        loss_value = loss(outputs, labels)\n",
    "        \n",
    "        \n",
    "        optimiser.step()\n",
    "    \n",
    "    print(loss_value)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "863cbb2df69581592effff968fe6db70",
     "grade": true,
     "grade_id": "cell-bad513af6ebf3103",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "data = np.linspace(-3, 3, 24).reshape(4, 6)\n",
    "loader = Dataloader(data, data, batch_size=2)\n",
    "net = Scale()\n",
    "loss = MeanSquaredErrorLoss()\n",
    "gd = GradientDescent(net.parameters(), lr=1e-2)\n",
    "result = update(net, loss, loader, gd)\n",
    "assert isinstance(result, np.ndarray), (\n",
    "    \"ex4: output of update is not a numpy array (-0.5 points)\"\n",
    ")\n",
    "assert result.size == 2, (\n",
    "    \"ex4: output of update does not hold scalar result for each mini-batch (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ea13c6915ecace241fe9fdc10f67bfa",
     "grade": true,
     "grade_id": "cell-30444cd7983175d6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30623819 2.75614367]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ex4: update function does not compute gradients for parameters (-1 point)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m gd \u001b[38;5;241m=\u001b[39m GradientDescent(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[0;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m update(net, loss, loader, gd)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(net\u001b[38;5;241m.\u001b[39mscale, \u001b[38;5;241m1.\u001b[39m), (\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex4: update function does not compute gradients for parameters (-1 point)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n",
      "\u001b[1;31mAssertionError\u001b[0m: ex4: update function does not compute gradients for parameters (-1 point)"
     ]
    }
   ],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "data = np.linspace(-3, 3, 24).reshape(4, 6)\n",
    "loader = Dataloader(data, 2 * data, batch_size=2)\n",
    "net = Scale()\n",
    "loss = MeanSquaredErrorLoss()\n",
    "gd = GradientDescent(net.parameters(), lr=1e-2)\n",
    "result = update(net, loss, loader, gd)\n",
    "assert not np.isclose(net.scale, 1.), (\n",
    "    \"ex4: update function does not compute gradients for parameters (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, network, loss, optimiser, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a neural network with gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_loader : Dataloader\n",
    "        Dataloader producing batches of input-target pairs.\n",
    "    valid_loader : Dataloader\n",
    "        Dataloader producing batches of input-target pairs.\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    loss : Module\n",
    "        Loss function module.\n",
    "    optimiser : Optimiser\n",
    "        Optimisation algorithm.\n",
    "    epochs : int, optional\n",
    "        Number of times to iterate the dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    train_errors : (epochs + 1, n_batches) ndarray\n",
    "        Training error for each epoch and each batch.\n",
    "    valid_errors : (epochs + 1, 1) ndarray\n",
    "        Validation error for each epoch.\n",
    "    \"\"\"\n",
    "    # log performance before training (for reference)\n",
    "    train_errors = [evaluate(network, loss.eval(), train_loader)]\n",
    "    valid_errors = [evaluate(network, loss.eval(), valid_loader)]\n",
    "    # train for given number of epochs\n",
    "    for _ in range(epochs):\n",
    "        train_errors.append(update(network, loss, train_loader, optimiser))\n",
    "        valid_errors.append(evaluate(network, loss.eval(), valid_loader))\n",
    "        \n",
    "    return np.stack(train_errors), np.stack(valid_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Training Logistic Regression (5 points)\n",
    "\n",
    "To test your framework, it is best to start with a simple problem.\n",
    "Therefore, we jump back to assignment one, where we learned that logistic regression is actually a single-layer network.\n",
    "If your optimiser works correctly, the loss should go down when a sufficiently small learning rate was chosen.\n",
    "\n",
    " > Build a single-layer network from your own modules and train it to classify MNIST digits.\n",
    " > You can use the `train` function from the previous exercise.\n",
    " > Train your network for at least five epochs.\n",
    " > Use an adaptive optimiser to train your network with *stochastic* gradients.\n",
    " > The `split_data` function can be used to create a validation dataset.\n",
    " \n",
    "**Hint:** the `Flatten` module provided earlier, might be useful for turning images into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29a7f99938f3cd4132c4f55e4331ebba",
     "grade": true,
     "grade_id": "cell-e1acdab4a9c5f59a",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "x_train, y_train = get_mnist_data()\n",
    "x_train, y_train = process_data(x_train, y_train)\n",
    "train_loader = Dataloader(x_train, y_train, batch_size=64, shuffle=True)\n",
    "\n",
    "x_valid, y_valid = get_mnist_data()  # Replace with your validation data loading logic\n",
    "x_valid, y_valid = process_data(x_valid, y_valid)\n",
    "valid_loader = Dataloader(x_valid, y_valid, batch_size=64, shuffle=False) \n",
    "\n",
    "# Build a single-layer network\n",
    "input_size = x_train.shape[1]\n",
    "output_size = 10  # Assuming 10 classes for MNIST\n",
    "lr = 0.001  # Choose an appropriate learning rate\n",
    "\n",
    "linear_layer = Linear(input_size, output_size)\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = GradientDescent([linear_layer.parameters()], lr=lr)\n",
    "\n",
    "# Train the network\n",
    "train_err, valid_err = train(train_loader, valid_loader, linear_layer, LossFunction(), optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(net, train_err, valid_err);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Training a Multi-layer Network (3 Points)\n",
    "\n",
    "It would have been silly to do all of this work to train logistic regression.\n",
    "Something you probably already would have been able to do before the start of this course.\n",
    "The real goal is to train *deep* networks with multiple layers.\n",
    "You probably can't wait to build a convolutional network with your framework!\n",
    "\n",
    "> Use one of the optimisers from above to train a multi-layer convolutional neural network on the MNIST dataset. \n",
    "> Feel free to also create new modules and try out new things! (make sure to include any new code in the notebook!).\n",
    "> Achieve a model with 80% accuracy to collect all points. \n",
    "> For reference: the sample solution (using the LeNet architecture illustrated below) takes &approx;15&nbsp;min to train for 10&nbsp;epochs.\n",
    "\n",
    "**Hint:** You can probably reuse a few things from the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <figcaption style=\"width: 100%; text-align: center;\">LeNet-5 architecture</figcaption>\n",
    "    <img src=\"https://miro.medium.com/max/2154/1*1TI1aGBZ4dybR6__DI9dzA.png\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledTanh(Tanh):\n",
    "    \n",
    "    def __init__(self, bound=1.7159, slope=2. / 3.):\n",
    "        super().__init__()\n",
    "        self.bound = bound\n",
    "        self.slope = slope\n",
    "    \n",
    "    def compute_outputs(self, x):\n",
    "        a, cache = super().compute_outputs(self.slope * x)\n",
    "        return self.bound * a, cache\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        return self.slope * super().compute_grads(self.bound * grads, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "336292e1015c511f690babe92c54d8f5",
     "grade": true,
     "grade_id": "cell-de1a7eb9a88f209f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m GradientDescent([linear_layer\u001b[38;5;241m.\u001b[39mparameters()], lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m train_err, valid_err \u001b[38;5;241m=\u001b[39m train(train_loader, valid_loader, [linear1, relu1, linear2, softmax], LossFunction(), optimizer, epochs\u001b[38;5;241m=\u001b[39myour_epochs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, y_train = get_mnist_data()\n",
    "x_train, y_train = process_data(x_train, y_train)\n",
    "train_loader = Dataloader(x_train, y_train, batch_size=64, shuffle=True)\n",
    "\n",
    "x_valid, y_valid = get_mnist_data()  # Replace with your validation data loading logic\n",
    "x_valid, y_valid = process_data(x_valid, y_valid)\n",
    "valid_loader = Dataloader(x_valid, y_valid, batch_size=64, shuffle=False) \n",
    "\n",
    "# Build a multi-layer convolutional neural network\n",
    "input_channels = 1  # Assuming grayscale images\n",
    "input_size = x_train.shape[1:]\n",
    "output_size = 10  # Assuming 10 classes for MNIST\n",
    "\n",
    "linear1 = Linear(input_channels * input_size[0] * input_size[1], 128)\n",
    "relu1 = ReLU()\n",
    "linear2 = Linear(128, output_size)\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = GradientDescent([linear_layer.parameters()], lr=lr)\n",
    "\n",
    "# Train the network\n",
    "train_err, valid_err = train(train_loader, valid_loader, [linear1, relu1, linear2, softmax], LossFunction(), optimizer, epochs=your_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_curves(cnn, train_err, valid_err);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9a56aac69e0f3fb4d9aac1f93b93113",
     "grade": false,
     "grade_id": "cell-cda3b08099d963ca",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "Is this what you expected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3e818e902ac19758a58e0f137f65c67",
     "grade": true,
     "grade_id": "cell-011a96516256e42f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
