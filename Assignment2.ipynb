{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  \n",
    "may  be  used  for  personal  and non-commercial educational use only.  \n",
    "Any reproduction of this manuscript, no matter whether as a whole or in parts, \n",
    "no matter whether in printed or in electronic form, \n",
    "requires explicit prior acceptance of the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 2 - WS 2023 -->\n",
    "\n",
    "# Multilayer Perceptrons (21 points + 2 bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the second assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with **Multi-Layer Perceptrons**. Essentially, MLPs are the result of stacking one or more of the simple networks from last time, interleaved with some form of non-linearities. This hints at some form of modular implementation, which will lead to the first building blocks of your very own deep learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from nnumpy import Module, Container, LossFunction\n",
    "from nnumpy.testing import gradient_check\n",
    "from nnumpy.utils import to_one_hot\n",
    "\n",
    "rng = np.random.default_rng(1856)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module System\n",
    "\n",
    "Multi-layer perceptrons can essentially be assembled from simple networks and activation functions. This kind of modularity is a recurring theme in deep learning, so much so that most deep learning frameworks are implemented in a modular fashion. This allows to construct complex networks from relatively simple building blocks. On top of that, it breaks down the possibly complex backprop derivation in more digestible pieces.\n",
    "\n",
    "In order to kick-start your deep learning library, we provide you with the `Module` class, which implements some plumbing and python magic in an attempt to make them easier to use. In order for your models to inherit this functionality, they should be subclasses of `Module` and implement the functions `compute_outputs` and `compute_grads`. The first function should compute the result of the forward pass and collect the values that will be necessary to compute the gradients in the backward pass. The backward pass is to be implemented in the second function, which comes down to computing all possible gradients. The following schematic illustrates this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, `compute_outputs` takes one or more inputs and should return two values: the output of the module from the input and the `cache`, which contains the values necessary for the backward pass. `compute_grads` expects the gradients from the module that uses the outputs from this module as inputs, as well as the `cache` from the forward pass as inputs. This function should compute the gradients w.r.t. the module's parameters, but also the gradients w.r.t. inputs, which are to be returned by `compute_grads`. See the example module `Example` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "288cf2fed097bd610927ed385a81651e",
     "grade": false,
     "grade_id": "cell-575baf03b2940b78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Square(Module):\n",
    "    \"\"\" Example of what a NNumpy module could look like. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, x):\n",
    "        return x * x, x\n",
    "    \n",
    "    def compute_grads(self, grads, x):\n",
    "        return grads * 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Activation Function Module (6 Points)\n",
    "\n",
    "Without non-linearities, Multi-Layer Perceptrons would make little sense. After all, multiple linear layers can always be reduced to a single-layer linear network, i.e. linear regression. Essentially any non-linear function could serve as activation function, but in practice only a few functions with specific properties are considered.\n",
    "\n",
    "An example of a function that *could* be used as activation function, but is rarely found in practice, is the following algebraic formulation of a sigmoidal function:\n",
    "\n",
    "$$\\sigma_\\mathrm{alg}(x) = \\frac{x}{\\sqrt{1 + x^2}}.$$\n",
    "\n",
    "When plotted, this function looks very similar to the more commonly used $\\tanh$.\n",
    "Although it is not a commonly used activation function, there is no (obvious) reason why this shouldn't work.\n",
    "\n",
    "Since most activation functions do not have any parameters, their modules are pretty straightforward to implement. Consider this first exercise as an opportunity to get familiar with the module system.\n",
    "\n",
    "> Implement the forward and backward pass for the `Identity`, `Tanh` and `AlgebraicSigmoid` activation function modules.\n",
    "> Use the formula above for the implementation of `AlgebraicSigmoid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "779d2701a9490887422dafbafd76b750",
     "grade": false,
     "grade_id": "cell-eda14d532b5d3fec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Identity(Module):\n",
    "    \"\"\" NNumpy implementation of the identity function. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return s,s\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return grads\n",
    "\n",
    "\n",
    "class Tanh(Module):\n",
    "    \"\"\" NNumpy implementation of the hyperbolic tangent function. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return np.tanh(s),s\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return (1 - np.tanh(cache) * np.tanh(cache)) * grads\n",
    "\n",
    "    \n",
    "class AlgebraicSigmoid(Module):\n",
    "    \"\"\" NNumpy implementation of an algebraic sigmoid function. \"\"\"\n",
    "\n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return s / np.sqrt(1 + s * s), s\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return (1 / np.sqrt(np.power(cache * cache + 1, 3))) * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "237484805c1a20eda465e47c2f975fd9",
     "grade": true,
     "grade_id": "cell-e329074b42d7b426",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "s = np.linspace(-3, 3, 35).reshape(7, 5)\n",
    "phi = Identity()\n",
    "a, cache = phi.compute_outputs(s)\n",
    "g = phi.compute_grads(np.ones_like(s), cache)\n",
    "assert isinstance(a, np.ndarray), (\n",
    "    \"ex1: output of Identity.compute_outputs is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert a.shape == s.shape, (\n",
    "    \"ex1: output of Identity.compute_outputs has incorrect shape (-1 point)\"\n",
    ")\n",
    "assert isinstance(g, np.ndarray), (\n",
    "    \"ex1: output of Identity.compute_grads is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert g.shape == s.shape, (\n",
    "    \"ex1: output of Identity.compute_grads has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12edcbf0faca6d9cf93cd01a0ee0c721",
     "grade": true,
     "grade_id": "cell-a6f45a6fd084f15c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "s = np.linspace(-3, 3, 35).reshape(7, 5)\n",
    "phi = Tanh()\n",
    "a, cache = phi.compute_outputs(s)\n",
    "assert isinstance(a, np.ndarray), (\n",
    "    \"ex1: output of Tanh.compute_outputs is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert a.shape == s.shape, (\n",
    "    \"ex1: output of Tanh.compute_outputs has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c68b12668939776543928f686f0d2447",
     "grade": true,
     "grade_id": "cell-de55ef90d1c433a5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "g = phi.compute_grads(np.ones_like(s), cache)\n",
    "assert isinstance(g, np.ndarray), (\n",
    "    \"ex1: output of Tanh.compute_grads is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert g.shape == s.shape, (\n",
    "    \"ex1: output of Tanh.compute_grads has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "790b2db4eb4d90ad5f16f2c4aec02823",
     "grade": true,
     "grade_id": "cell-8247f4348d2eaee4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "s = np.linspace(-3, 3, 35).reshape(7, 5)\n",
    "phi = AlgebraicSigmoid()\n",
    "a, cache = phi.compute_outputs(s)\n",
    "assert isinstance(a, np.ndarray), (\n",
    "    \"ex1: output of AlgebraicSigmoid.compute_outputs is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert a.shape == s.shape, (\n",
    "    \"ex1: output of AlgebraicSigmoid.compute_outputs has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc53481d4eb0afcb5ec4ed277a7074cc",
     "grade": true,
     "grade_id": "cell-049495398745f8f1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "g = phi.compute_grads(np.ones_like(s), cache)\n",
    "assert isinstance(g, np.ndarray), (\n",
    "    \"ex1: output of Tanh.compute_grads is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert g.shape == s.shape, (\n",
    "    \"ex1: output of Tanh.compute_grads has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b35ec96f97a64864b70f824aec910b3",
     "grade": true,
     "grade_id": "cell-d00d3fe772bd69c6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "It is not uncommon to make mistakes when computing gradients - or implementing them - in neural networks. In order to catch possible issues with the backward pass in deep learning, a technique called *gradient checking* is often used. To check the implementation of an analytically derived gradient, a numerical approximation is used to check the analytic gradient implementation against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different methods to approximate gradients numerically, but one of the easiest is probably to use finite difference approximations. This method directly uses the definition of a derivative, \n",
    "\n",
    "$$f'(x) = \\lim_\\limits{h \\to 0} \\frac{f(x + h) - f(x)}{h},$$\n",
    "\n",
    "which can also be written as\n",
    "\n",
    "$$f'(x) = \\lim_\\limits{h \\to 0} \\frac{f(x) - f(x - h)}{h}$$\n",
    "\n",
    "or, for the sake of symmetry, as the average of the two expressions above:\n",
    "\n",
    "$$f'(x) = \\lim_\\limits{h \\to 0} \\frac{f(x + h) - f(x - h)}{2h}.$$\n",
    "\n",
    "In the end, a finite difference approximation is nothing more than one of the above formulas without limits, i.e. for some value of $h$. In practice, $h$ is set to some small value, often referred to as $\\varepsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Gradient Checking (3 Points)\n",
    "  \n",
    "Although gradient checking normally does not appear in the interface of deep learning frameworks, it is an important tool to make sure implementations are correct. Therefore, we will start off by implementing a very simple gradient checker.\n",
    "\n",
    "> Implement the `numeric_grad` function and use it to write a simple gradient checker (`my_gradient_check`) that is able to check the gradients of simple modules like the activation functions from the previous exercise.\n",
    "\n",
    "**Hint:** Gradient checks for *element-wise* functions are considerably easier than gradient checks for multi-variable vector functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c8d4e932156840efb4346b90fe59a87",
     "grade": false,
     "grade_id": "cell-151340418a185c94",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def numeric_grad(func, x, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Compute the numerical gradient for a(n element-wise) function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable\n",
    "        The function to compute the difference quotient for.\n",
    "    x : ndarray\n",
    "        The points in which the gradient should be computed.\n",
    "    eps : float, optional\n",
    "        The epsilon perturbation used in the numerical computation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    grad : ndarray\n",
    "        The numerically computed gradient.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return (func(x + eps) - func(x)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11f826d3f0387e171d48dbc7092bdda9",
     "grade": false,
     "grade_id": "cell-c4524bc1b1e4cb4e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_gradient_check(module, x, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Check if the input gradients of an activation function module are correct.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    module : Module\n",
    "        The activation function module to check.\n",
    "    x : ndarray\n",
    "        The points in which the gradient should be computed.\n",
    "    eps : float, optional\n",
    "        The small perturbation used in the numerical computation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    success : bool\n",
    "        Whether or not the gradient check passed.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    \n",
    "    nums = numeric_grad(module,x,eps)\n",
    "    gradients = np.ones(x.shape)\n",
    "    grads = module.compute_grads(gradients, x)\n",
    "    \n",
    "\n",
    "    if np.allclose(grads,nums):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d205c0504c771ee3d5a5a42d16db714",
     "grade": true,
     "grade_id": "cell-00fa2302c4c29815",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = np.linspace(-5, 5)\n",
    "dx = numeric_grad(lambda _x: _x ** 3, x)\n",
    "assert isinstance(dx, np.ndarray), (\n",
    "    \"ex2: output of numeric_grad is not a numpy array (-0.5 points)\"\n",
    ")\n",
    "assert dx.shape == x.shape, (\n",
    "    \"ex2: output of numeric_grad has incorrect shape (-0.5 points)\"\n",
    ")\n",
    "assert np.allclose(dx, 3 * x ** 2), (\n",
    "    \"ex2: numeric_grad does not compute reasonable approximation (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdf59673caf87cfe0e4e57f8838f8983",
     "grade": true,
     "grade_id": "cell-2690a34a6b8e23a9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = np.linspace(-5, 5)\n",
    "check = my_gradient_check(Square(), x)\n",
    "assert check, (\n",
    "    \"ex3: my_gradient_check did not pass for Square module (-1 point)\"\n",
    ")\n",
    "\n",
    "class BadSquare(Square):\n",
    "    def compute_grads(self, grads, x):\n",
    "        return grads * x\n",
    "\n",
    "check = my_gradient_check(BadSquare(), x)\n",
    "assert not check, (\n",
    "    \"ex3: my_gradient_check passed for BadSquare module (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ee09b44ef11d679a712c3693d90ec85",
     "grade": true,
     "grade_id": "cell-aefb69264fcf9aae",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a9572ff171cc739d92443fb8ae58f93",
     "grade": true,
     "grade_id": "cell-e8049d4b414ec72b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cad76cf489ab3ab3801541fe8f01d1d",
     "grade": true,
     "grade_id": "cell-bcb0e11fd842015b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a933708bd87345d5cc636befd2621fc",
     "grade": true,
     "grade_id": "cell-6ae1e1e54f7997c8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check for Identity: passed\n",
      "gradient check for Tanh:     passed\n",
      "gradient check for Sigmoid:  passed\n"
     ]
    }
   ],
   "source": [
    "check = my_gradient_check(Identity(), np.linspace(-3, 3))\n",
    "print(\"gradient check for Identity:\", \"passed\" if check else \"failed\")\n",
    "check = my_gradient_check(Tanh(), np.linspace(-3, 3))\n",
    "print(\"gradient check for Tanh:    \", \"passed\" if check else \"failed\")\n",
    "check = my_gradient_check(AlgebraicSigmoid(), np.linspace(-3, 3))\n",
    "print(\"gradient check for Sigmoid: \", \"passed\" if check else \"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer\n",
    "\n",
    "One of the key components in a multi-layer perceptron is the fully connected layer. It can be implemented in a fairly simple module with a weight matrix and bias vector as parameters. The `Module` base-class provides a method `Module.register_parameter(name, value)` that auto-magically creates an attribute `Module.<name>` for the module. This attribute has the type `Parameter`, which is essentiall a numpy array with an additional attribute `Parameter.grad` to store its gradients. In order to make sure that the parameters are correctly initialised, use the method `Module.reset_parameters()`. For initialising the gradients, there is also a method, `Module.zero_grad()`, that should be called after every update. Take the following code as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.20704785e-311 0.00000000e+000 4.94065646e-324] [0. 0. 0.]\n",
      "None [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "m = Module()\n",
    "\n",
    "# create attributes\n",
    "m.register_parameter('theta', np.empty(3))\n",
    "\n",
    "# parameter attribute (Note that parameters are uninitialised!)\n",
    "print(m.theta, end=' ')\n",
    "m.reset_parameters()  # initialise parameters\n",
    "print(m.theta)\n",
    "\n",
    "# parameter gradient attribute\n",
    "print(m.theta.grad, end=' ')\n",
    "m.zero_grad()\n",
    "print(m.theta.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: A Module with Parameters (6 Points)\n",
    "\n",
    "Remember the linear regression from the first assignment? It's time to pour that code into a module and check whether you can correctly propagate gradients! \n",
    "\n",
    "> Implement the `Linear` module, which represents a fully connected layer. The derivatives w.r.t. the parameters can be stored in the `grad` attribute of registered parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5bcdbd037a269f012165261b024fd44",
     "grade": false,
     "grade_id": "cell-ac48e509ce86273b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    NNumpy implementation of a fully connected layer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features (D) this layer expects.\n",
    "    out_features : int\n",
    "        Number of output features (K) this layer expects.\n",
    "    use_bias : bool\n",
    "        Flag to indicate whether the bias parameters are used.\n",
    "\n",
    "    w : Parameter\n",
    "        Weight matrix.\n",
    "    b : Parameter\n",
    "        Bias vector.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> fc = Linear(10, 1)\n",
    "    >>> fc.reset_parameters()  # init parameters\n",
    "    >>> s = fc.forward(np.random.randn(1, 10))\n",
    "    >>> fc.zero_grad()  # init parameter gradients\n",
    "    >>> ds = fc.backward(np.ones_like(s))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # register parameters 'w' and 'b' here (mind use_bias!)\n",
    "        # YOUR CODE HERE\n",
    "        self.register_parameter(\"w\", np.zeros((self.out_features, self.in_features)))\n",
    "        if(self.use_bias):\n",
    "            self.register_parameter(\"b\", np.zeros((self.out_features)))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self, seed: int = None):\n",
    "        \"\"\" \n",
    "        Reset the parameters to some random values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int, optional\n",
    "            Seed for random initialisation.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.w = rng.standard_normal(size=self.w.shape)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros_like(self.b)\n",
    "    \n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, D) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        s : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        N = x.shape[0]\n",
    "        D = x.shape[1]\n",
    "        K = self.w.shape[0]\n",
    "        s = np.zeros((N,K))\n",
    "        \n",
    "        if(self.use_bias):\n",
    "            #s = x @ self.w.T + self.b\n",
    "            s = np.dot(x,self.w.T) + self.b\n",
    "        else:\n",
    "            #s = x @ self.w.T\n",
    "            s = np.dot(x,self.w.T)\n",
    "            \n",
    "        cache = x\n",
    "        return s, cache\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, D) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        D = self.w.shape[1]\n",
    "        N = grads.shape[0]\n",
    "        K = grads.shape[1]\n",
    "        dx = np.zeros((N,D))\n",
    "        \n",
    "        #dx = grads @ self.w\n",
    "        dx = np.dot(grads,self.w)\n",
    "        #dw = grads.T @ cache\n",
    "        dw = np.dot(grads.T,cache)\n",
    "        if(self.use_bias):\n",
    "            db = np.sum(grads, axis=0)\n",
    "        else:\n",
    "            db = np.zeros(self.b.shape)\n",
    "            \n",
    "        self.b.grad = db\n",
    "        self.w.grad = dw\n",
    "        \n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b74f62087b7498920b34047c45fc494e",
     "grade": true,
     "grade_id": "cell-ee6fc36fea675723",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "lin = Linear(7, 17, use_bias=False)\n",
    "parameter_names = dict(lin.named_parameters())\n",
    "assert \"w\" in parameter_names, (\n",
    "    \"ex3: Linear module does not have 'w' parameter (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56247a4f420cf5001f96c4d6eb794909",
     "grade": true,
     "grade_id": "cell-8790ed616c945698",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "lin = Linear(19, 7, use_bias=True)\n",
    "parameter_names = dict(lin.named_parameters())\n",
    "assert \"w\" in parameter_names, (\n",
    "    \"ex3: Linear module does not have 'w' parameter (-1 point)\"\n",
    ")\n",
    "assert \"b\" in parameter_names, (\n",
    "    \"ex3: Linear module does not have 'b' parameter (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fddf510557a2d0137a69c2a3e497bbb4",
     "grade": true,
     "grade_id": "cell-0a11f7964ebbf1b0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = rng.normal(size=(11, 19))\n",
    "s, cache = lin.compute_outputs(x)\n",
    "assert isinstance(s, np.ndarray), (\n",
    "    \"ex3: output of Linear.compute_outputs is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert s.shape == (len(x), lin.out_features), (\n",
    "    \"ex3: output of Linear.compute_outputs has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a900936623f014426d9ba6af08eee438",
     "grade": true,
     "grade_id": "cell-8e44e38e3329bf43",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "lin.zero_grad()\n",
    "g = lin.compute_grads(np.ones_like(s), cache)\n",
    "assert isinstance(g, np.ndarray), (\n",
    "    \"ex3: output of Linear.compute_grads is not a numpy array (-0.5 points)\"\n",
    ")\n",
    "assert g.shape == x.shape, (\n",
    "    \"ex3: output of Linear.compute_grads has incorrect shape (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bee5095e0baa4594085531f67500af4",
     "grade": true,
     "grade_id": "cell-09047cbbe9a319c0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert np.nonzero(lin.w.grad), (\n",
    "    \"ex3: Linear.compute_grads does not compute gradients for 'w' parameter (-0.5 points)\"\n",
    ")\n",
    "assert np.nonzero(lin.b.grad), (\n",
    "    \"ex3: Linear.compute_grads does not compute gradients for 'b' parameter (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f0af7155677a276251359ed13e598fd",
     "grade": true,
     "grade_id": "cell-a4fc5ce25b5fd542",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert gradient_check(lin, x, debug=True), (\n",
    "    \"ex3: Linear module does not pass gradient check (-2 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "A multi-layer perceptron is essentially a stack of single-layer networks with some sort of non-linearity. With the `Linear` module and activation function modules, we have all ingredients to construct MLPs and make learning *deep*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Chaining modules (3 Points)\n",
    "\n",
    "In essence, MLPs can be constructed by chaining the modules in the right order. Since this is a common pattern in deep learning architectures, it makes sense to make a general module for chaining other modules. \n",
    "\n",
    "> Implement forward and backward pass for the `Sequential` module so that it comes down to chaining all its sub-modules.\n",
    "> Make sure that your implementation does not have any side-effects &mdash; i.e. your solution should **not** use `Module.forward` or `Module.backward`.\n",
    "\n",
    "**Hint:** the cache will probably not be a single numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "162213cca266889f9df34d3ad398bfd8",
     "grade": false,
     "grade_id": "cell-66c6d1c50fb0c9e8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sequential(Container):\n",
    "    \"\"\"\n",
    "    NNumpy module that chains together multiple one-to-one sub-modules.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    Doubling a module could be done as follows:\n",
    "    >>> module = Module()\n",
    "    >>> seq = Sequential(module, module)\n",
    "    \n",
    "    Modules can be accessed by index or by iteration:\n",
    "    >>> assert module is seq[0] and module is seq[1]\n",
    "    >>> mod1, mod2 = (m for m in seq)\n",
    "    >>> assert mod1 is module and mod2 is module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *modules):\n",
    "        super().__init__()\n",
    "        if len(modules) == 1 and hasattr(modules[0], '__iter__'):\n",
    "            modules = modules[0]\n",
    "        \n",
    "        for mod in modules:\n",
    "            self.add_module(mod)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, D) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        cache = []\n",
    "        y = []\n",
    "        current_x = x\n",
    "        current_cache = []\n",
    "\n",
    "        for mod in self._modules:\n",
    "            current_x, current_cache = mod.compute_outputs(current_x.T)\n",
    "            cache.append(current_cache)\n",
    "            \n",
    "        y = current_x\n",
    "        return y, cache\n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, D) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        self._modules.reverse()\n",
    "        cache.reverse()\n",
    "        current_dx = grads\n",
    "        \n",
    "        for mod,c in zip(self._modules,cache):\n",
    "            current_dx = mod.compute_grads(current_dx, c)\n",
    "            current_dx = current_dx.T\n",
    "        \n",
    "        dx = current_dx\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad69cf71be35a301692682767477259",
     "grade": false,
     "grade_id": "cell-add09daea829c91f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example usage\n",
    "class MLP(Sequential):\n",
    "    \"\"\" NNumpy implementation of Multi-Layer Perceptron. \"\"\"\n",
    "    \n",
    "    def __init__(self, *features, act_func=None, use_bias=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        f1, f2, ..., fn : int\n",
    "            Number of neurons in each layer.\n",
    "            f1 is the number of neurons in the input-layer.\n",
    "            fn is the number of neurons in the output-layer.\n",
    "        act_func : Module, optional\n",
    "            Module to use as activation function.\n",
    "            If not specified, the model is linear.\n",
    "        use_bias : bool, optional\n",
    "            Whether or not each layer should have a bias term.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = features[0]\n",
    "        self.out_features = features[-1]\n",
    "        self.use_bias = use_bias\n",
    "        self.phi = act_func or Identity()\n",
    "        \n",
    "        for n_in, n_out in zip(features[:-2], features[1:-1]):\n",
    "            self.add_module(Linear(n_in, n_out, use_bias))\n",
    "            self.add_module(self.phi)\n",
    "        \n",
    "        self.output_layer = Linear(features[-2], features[-1], use_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1217bd3a74213198d0dcb31bd5c8ea5c",
     "grade": true,
     "grade_id": "cell-0aa88cdc4fe4c0cc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "mlp = MLP(19, 7, 11, 3, act_func=Tanh())\n",
    "x = rng.normal(size=(11, 19))\n",
    "logits, cache = mlp.compute_outputs(x)\n",
    "assert isinstance(logits, np.ndarray), (\n",
    "    \"ex4: output of Sequential.compute_outputs is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert logits.shape == (len(x), 3), (\n",
    "    \"ex4: output of Sequential.compute_outputs has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bcc41d47aae8c51d14a94d8f47ae4a0f",
     "grade": true,
     "grade_id": "cell-f54292a3847f744d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "mlp.zero_grad()\n",
    "g = mlp.compute_grads(np.ones_like(logits), cache)\n",
    "assert isinstance(g, np.ndarray), (\n",
    "    \"ex4: output of Sequential.compute_grads is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert g.shape == x.shape, (\n",
    "    \"ex4: output of Sequential.compute_grads has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9410922c43169dde0ee7f795980e9ee",
     "grade": true,
     "grade_id": "cell-3fa09f24d9e81f78",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (19,3) and (7,11) not aligned: 3 (dim 1) != 7 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test Cell: do not edit or delete!\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m gradient_check(mlp, x, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), (\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mex4: Sequential module does not pass gradient check (-1 point)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n",
      "File \u001b[1;32m~\\Desktop\\nnumpy\\testing.py:92\u001b[0m, in \u001b[0;36mgradient_check\u001b[1;34m(module, eps, debug, chain_rule, *inputs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# analytical gradients\u001b[39;00m\n\u001b[0;32m     91\u001b[0m module\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 92\u001b[0m pred \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     93\u001b[0m backprop_seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m*\u001b[39mpred\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mif\u001b[39;00m chain_rule \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones_like(pred)\n\u001b[0;32m     94\u001b[0m dx_analytic \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mbackward(backprop_seed)\n",
      "File \u001b[1;32m~\\Desktop\\nnumpy\\__init__.py:300\u001b[0m, in \u001b[0;36mModule.forward\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03mForward pass through this module.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03mthe values that are necessary for gradient computation automagically.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape_cache\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtuple\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs))\n\u001b[1;32m--> 300\u001b[0m out, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_outputs(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\u001b[38;5;241m.\u001b[39mappend(cache)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m, in \u001b[0;36mSequential.compute_outputs\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m current_cache \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m---> 43\u001b[0m     current_x, current_cache \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mcompute_outputs(current_x\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     44\u001b[0m     cache\u001b[38;5;241m.\u001b[39mappend(current_cache)\n\u001b[0;32m     46\u001b[0m y \u001b[38;5;241m=\u001b[39m current_x\n",
      "Cell \u001b[1;32mIn[20], line 75\u001b[0m, in \u001b[0;36mLinear.compute_outputs\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     71\u001b[0m s \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N,K))\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m#s = x @ self.w.T + self.b\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     s \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m#s = x @ self.w.T\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     s \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (19,3) and (7,11) not aligned: 3 (dim 1) != 7 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert gradient_check(mlp, x, debug=True), (\n",
    "    \"ex4: Sequential module does not pass gradient check (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Necessity for Non-linearity (1 Point)\n",
    "\n",
    "As already mentioned, a multi-layer perceptron without non-linearities is equivalent to a single-layer network. How is that?\n",
    "\n",
    "> Implement the `extract_linear` function so that it returns a single linear layer that produces the same results as the given multi-layer linear network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dccf5c96b6729383e7d2ec7f4c9ade9a",
     "grade": false,
     "grade_id": "cell-5dcd45388f8899e6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_linear(layers: Sequential) -> Linear:\n",
    "    \"\"\"\n",
    "    Extract a single-layer linear layer from a multi-layer linear network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    layers : Sequential\n",
    "        A sequential module with multiple linear layers.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    slp : Linear\n",
    "        A linear module that is equivalent to the given sequential model.\n",
    "        I.e. a single layer that produces the same outputs.\n",
    "    \"\"\"\n",
    "    slp = Linear(layers[0].in_features, layers[-1].out_features)\n",
    "    slp.w = np.nan\n",
    "    slp.b = np.nan\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e1b65487e107683c3361eecb270cd1c",
     "grade": true,
     "grade_id": "cell-780f829a93ca409d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "x = np.linspace(-3, 3, 23).reshape(1, -1)\n",
    "linear_mlp = Sequential(\n",
    "    Linear(23, 17),\n",
    "    Linear(17, 19),\n",
    "    Linear(19, 7)\n",
    ")\n",
    "\n",
    "slp = extract_linear(linear_mlp)\n",
    "\n",
    "assert not np.any(np.isnan(slp.w)), (\n",
    "    \"ex5: no solution for slp.w (-0.5 points)\"\n",
    ")\n",
    "assert np.allclose(slp(x), linear_mlp(x)), (\n",
    "    \"ex5: slp.w has incorrect values (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f80e64828878fee43bdfbde877f81d3b",
     "grade": true,
     "grade_id": "cell-8de2590837885780",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert not np.any(np.isnan(slp.b)), (\n",
    "    \"ex5: no solution for slp.b (-0.5 points)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Modules\n",
    "\n",
    "Also loss functions fit in the module system. Loss functions take two inputs and can normally be returned in one of three ways:\n",
    "\n",
    "  1. An array of individual sample errors\n",
    "  2. The total sample error(s)\n",
    "  3. The average sample error(s)\n",
    "  \n",
    "To facilitate these three options, a `LossFunction` module has been provided. This module accepts a keyword argument `reduction` that allows to specify how to *reduce* the errors for different sampels (options are respectively `'none'`, `'sum'` or `'mean'`). For gradient checking, you want to use `'none'`, but in practice it is more common to use something like `'mean'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Logit Cross-Entropy (2 Points + 2 Bonus Points)\n",
    "\n",
    "The last assignment required you to implement the `softmax` and `cross_entropy` functions separately. In most deep learning frameworks, these functions are merged into a single loss function. The main reason is numerical stability, but it also simplifies the computation of the gradients.\n",
    "\n",
    "> Implement a module, i.e. forward and backward pass, that computes the cross-entropy from pre-activations and one-hot targets. E.g. by first computing softmax and then applying cross-entropy. Don't forget to compute the gradients w.r.t. targets! \n",
    "\n",
    "For the bonus point, you would have to\n",
    "\n",
    "> implement the numerically stable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8227577a21c0f17ec2b238a7f31b72f1",
     "grade": false,
     "grade_id": "cell-2f233ffa04514ddc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogitCrossEntropy(LossFunction):\n",
    "    \"\"\"\n",
    "    NNumpy implementation of the cross entropy loss function\n",
    "    computed from the logits, i.e. before applying the softmax nonlinearity.\n",
    "    \"\"\"\n",
    "\n",
    "    def raw_outputs(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Computation of loss without reduction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : (N, K) ndarray\n",
    "        targets : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        cross_entropy : (N, ) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def raw_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Computation of gradients for loss without reduction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, ) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dlogits : (N, K) ndarray\n",
    "        dtargets : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a44332e2e7e947cda5966fa9e640b90d",
     "grade": true,
     "grade_id": "cell-6486b9275ca3bf6a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "lce = LogitCrossEntropy(reduction=\"none\")\n",
    "y_hat = rng.normal(size=(11, 7))\n",
    "targets = to_one_hot(rng.integers(7, size=11), k=7)\n",
    "l, cache = lce.raw_outputs(y_hat, targets)\n",
    "assert isinstance(l, np.ndarray), (\n",
    "    \"ex6: output of LogitCrossEntropy.raw_outputs is not a numpy array (-1 point)\"\n",
    ")\n",
    "assert l.shape == (len(y_hat), ), (\n",
    "    \"ex6: output of LogitCrossEntropy.raw_outputs has incorrect shape (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80e38a52f418fe2f604406aaee3f2a05",
     "grade": true,
     "grade_id": "cell-a3d8c8745f3b9eb8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "assert gradient_check(lce, y_hat, targets, debug=True), (\n",
    "    \"ex6: LogitCrossEntropy module does not pass gradient check (-1 point)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b546376073f58c0c9e8f4b8ad655739",
     "grade": true,
     "grade_id": "cell-a6608651bd423957",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Test Cell: do not edit or delete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
